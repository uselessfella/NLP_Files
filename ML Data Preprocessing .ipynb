{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab1415fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the necessary library.\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "sn_stemmer=SnowballStemmer(\"english\")\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0e2f7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def datacleaning(x):\n",
    "#---------------------------------------------------------------------------------------------------------------------------    \n",
    "    #Removing stop words.\n",
    "    y=nltk.word_tokenize(x)\n",
    "    temp=[]\n",
    "    for i in y:\n",
    "          if i in stopwords.words(\"english\"): \n",
    "            pass\n",
    "          else:\n",
    "            temp.append(i)\n",
    "    my_new_string=' '.join(temp)\n",
    "#---------------------------------------------------------------------------------------------------------------------------        \n",
    "    #Lemmatizing the data.\n",
    "    c=nltk.word_tokenize(my_new_string)\n",
    "    my_list=[]\n",
    "    for i in c:\n",
    "        my_list.append(lemmatizer.lemmatize(i))\n",
    "        my_new_string_second=' '.join(my_list)\n",
    "#---------------------------------------------------------------------------------------------------------------------------            \n",
    "    #Stemming the data.\n",
    "        d=nltk.word_tokenize(my_new_string_second)\n",
    "    my_list_stemmer=[]\n",
    "    for i in d:\n",
    "        my_list_stemmer.append(sn_stemmer.stem(i))\n",
    "        final=' '.join(my_list_stemmer)\n",
    "#---------------------------------------------------------------------------------------------------------------------------  \n",
    "    #Removing the numbers from the data.\n",
    "        temp=[]\n",
    "    for i in final:\n",
    "        if i.isdigit(): \n",
    "             pass\n",
    "        else:\n",
    "            temp.append(i)\n",
    "    hi=''.join(temp)\n",
    "    return (hi.lower()) #Converting the data to lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f3b8746",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Restaurant_Reviews - Restaurant_Reviews.tsv',sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0607efb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Cleaned_review']=df['Review'].apply(datacleaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "185b2a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a4dd02f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_y=df['Liked']\n",
    "df_x=df['Cleaned_review']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d120fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test=train_test_split(df_x,df_y,test_size=0.33,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "05d8fcba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[576                 i swung give tri deepli disappoint .\n",
       " 825                                     wo n't ever go .\n",
       " 41         there deal good enough would drag establish .\n",
       " 635                               we prompt greet seat .\n",
       " 560     my salad bland vinegrett babi green heart palm .\n",
       "                              ...                        \n",
       " 766            one place phoenix i would defin go back .\n",
       " 421    on ground , right next tabl larg , smear , bee...\n",
       " 595           the crouton also tast homemad extra plus .\n",
       " 737    tri airport experi tasti food speedi , friend ...\n",
       " 408                                     servic fantast .\n",
       " Name: Cleaned_review, Length: 800, dtype: object,\n",
       " 809    the first time i ever came i amaz experi , i s...\n",
       " 467    this due fact took  minut acknowledg , anoth  ...\n",
       " 755        when i 'm side town , definit spot i ll hit !\n",
       " 566    the server went back forth sever time , even m...\n",
       " 526                                     and way expens .\n",
       "                              ...                        \n",
       " 8                                        the fri great .\n",
       " 479                                             i love !\n",
       " 208                                   must night place .\n",
       " 211    for  minut , re wait salad realiz n't come tim...\n",
       " 270                      the veggitarian platter world !\n",
       " Name: Cleaned_review, Length: 200, dtype: object,\n",
       " 576    0\n",
       " 825    0\n",
       " 41     0\n",
       " 635    1\n",
       " 560    0\n",
       "       ..\n",
       " 766    1\n",
       " 421    0\n",
       " 595    1\n",
       " 737    1\n",
       " 408    1\n",
       " Name: Liked, Length: 800, dtype: int64,\n",
       " 809    1\n",
       " 467    0\n",
       " 755    1\n",
       " 566    0\n",
       " 526    0\n",
       "       ..\n",
       " 8      1\n",
       " 479    1\n",
       " 208    0\n",
       " 211    0\n",
       " 270    1\n",
       " Name: Liked, Length: 200, dtype: int64]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_test_split(df_x,df_y,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f1e43a08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer()"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "#initializing the vector.\n",
    "hi_vectorize = CountVectorizer()\n",
    "hi_vectorize.fit(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f0cac59c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['absolut', 'absolutley', 'accid', 'accomod', 'accord', 'account',\n",
       "       'acknowledg', 'actual', 'ad', 'afford', 'after', 'afternoon',\n",
       "       'again', 'ago', 'al', 'ala', 'all', 'allergi', 'almost', 'alon',\n",
       "       'also', 'although', 'alway', 'amaz', 'ambianc', 'ambienc',\n",
       "       'amount', 'ampl', 'an', 'and', 'andddd', 'ani', 'anoth', 'anymor',\n",
       "       'anyon', 'anyth', 'anytim', 'anyway', 'apart', 'apolog', 'app',\n",
       "       'appal', 'appar', 'appeal', 'appet', 'appetit', 'appl', 'are',\n",
       "       'area', 'arepa', 'aria', 'around', 'array', 'arriv', 'as', 'ask',\n",
       "       'assur', 'at', 'ate', 'atmospher', 'atmosphere', 'attach',\n",
       "       'attack', 'attent', 'attitud', 'auju', 'authent', 'averag',\n",
       "       'avocado', 'avoid', 'aw', 'away', 'awesom', 'awkward', 'babi',\n",
       "       'back', 'bacon', 'bad', 'bagel', 'bakeri', 'bamboo', 'bar', 'bare',\n",
       "       'bargain', 'bartend', 'basebal', 'basic', 'bathroom', 'batter',\n",
       "       'bay', 'bbq', 'bean', 'beat', 'beauti', 'becom', 'beef', 'been',\n",
       "       'beer', 'begin', 'behind'], dtype=object)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hi_vectorize.get_feature_names_out()[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "723dba7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_hi = hi_vectorize.transform(x_train)\n",
    "x_train_vector=x_train_hi.toarray()\n",
    "x_train_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4d19ad29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer()"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hi_vectorize.fit(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9ebba602",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['accommod', 'ach', 'acknowledg', 'across', 'actual', 'after',\n",
       "       'ahead', 'airlin', 'airport', 'albondiga', 'all', 'almond', 'also',\n",
       "       'although', 'alway', 'amaz', 'ambienc', 'amount', 'an', 'and',\n",
       "       'angri', 'annoy', 'anoth', 'anticip', 'anytim', 'approv', 'area',\n",
       "       'around', 'arriv', 'articl', 'as', 'ask', 'at', 'atmospher',\n",
       "       'atroci', 'attent', 'authent', 'averag', 'avoid', 'aw', 'awesom',\n",
       "       'ayc', 'az', 'baba', 'bachi', 'back', 'bacon', 'bad', 'baklava',\n",
       "       'ball', 'banana', 'bank', 'bar', 'bare', 'bartend', 'base',\n",
       "       'batch', 'bathroom', 'batter', 'bay', 'be', 'beateous', 'beauti',\n",
       "       'beef', 'beer', 'befor', 'behind', 'bellagio', 'best', 'better',\n",
       "       'beyond', 'big', 'bigger', 'bing', 'bit', 'bite', 'black', 'bland',\n",
       "       'blandest', 'block', 'bloddi', 'bloodiest', 'blow', 'boba', 'boil',\n",
       "       'both', 'box', 'boyfriend', 'bread', 'breakfast', 'brisket',\n",
       "       'brought', 'brushfir', 'buck', 'buffet', 'bug', 'burger', 'burn',\n",
       "       'busi', 'but'], dtype=object)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hi_vectorize.get_feature_names_out()[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7f119d13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test_hi = hi_vectorize.transform(x_test)\n",
    "x_test_vector=x_test_hi.toarray()\n",
    "x_test_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8a9f3e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le=LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f39dd6ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le.fit(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e0a0a424",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1], dtype=int64)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f1f5949c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train=le.transform(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9e2b6edb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "703    1\n",
       "311    0\n",
       "722    1\n",
       "629    1\n",
       "0      1\n",
       "      ..\n",
       "106    1\n",
       "270    1\n",
       "860    1\n",
       "435    0\n",
       "102    1\n",
       "Name: Liked, Length: 670, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "07f3c5a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le.fit(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c0d17777",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1], dtype=int64)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a2edcff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test=le.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "02a478d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1,\n",
       "       0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0,\n",
       "       1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1,\n",
       "       1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0,\n",
       "       1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
       "       1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1,\n",
       "       0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0,\n",
       "       1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0,\n",
       "       0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1,\n",
       "       0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
       "       1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0,\n",
       "       1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0,\n",
       "       1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
