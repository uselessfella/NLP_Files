{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4daeda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "sn_stemmer=SnowballStemmer(\"english\")\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1bc764c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Restaurant_Reviews - Restaurant_Reviews.tsv',sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44ef4694",
   "metadata": {},
   "outputs": [],
   "source": [
    "def datacleaning(x):\n",
    "#---------------------------------------------------------------------------------------------------------------------------    \n",
    "    #Removing stop words.\n",
    "    y=nltk.word_tokenize(x)\n",
    "    temp=[]\n",
    "    for i in y:\n",
    "          if i in stopwords.words(\"english\"): \n",
    "            pass\n",
    "          else:\n",
    "            temp.append(i)\n",
    "    my_new_string=' '.join(temp)\n",
    "#---------------------------------------------------------------------------------------------------------------------------        \n",
    "    #Lemmatizing the data.\n",
    "    c=nltk.word_tokenize(my_new_string)\n",
    "    my_list=[]\n",
    "    for i in c:\n",
    "        my_list.append(lemmatizer.lemmatize(i))\n",
    "        my_new_string_second=' '.join(my_list)\n",
    "#---------------------------------------------------------------------------------------------------------------------------            \n",
    "    #Stemming the data.\n",
    "        d=nltk.word_tokenize(my_new_string_second)\n",
    "    my_list_stemmer=[]\n",
    "    for i in d:\n",
    "        my_list_stemmer.append(sn_stemmer.stem(i))\n",
    "        final=' '.join(my_list_stemmer)\n",
    "#---------------------------------------------------------------------------------------------------------------------------  \n",
    "    #Removing the numbers from the data.\n",
    "        temp=[]\n",
    "    for i in final:\n",
    "        if i.isdigit(): \n",
    "             pass\n",
    "        else:\n",
    "            temp.append(i)\n",
    "    hi=''.join(temp)\n",
    "    return (hi.lower()) #Converting the data to lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30ef0ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Cleaned_review']=df['Review'].apply(datacleaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68197140",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Liked</th>\n",
       "      <th>Cleaned_review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wow... Loved this place.</td>\n",
       "      <td>1</td>\n",
       "      <td>wow ... love place .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Crust is not good.</td>\n",
       "      <td>0</td>\n",
       "      <td>crust good .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Not tasty and the texture was just nasty.</td>\n",
       "      <td>0</td>\n",
       "      <td>not tasti textur nasti .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Stopped by during the late May bank holiday of...</td>\n",
       "      <td>1</td>\n",
       "      <td>stop late may bank holiday rick steve recommen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The selection on the menu was great and so wer...</td>\n",
       "      <td>1</td>\n",
       "      <td>the select menu great price .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>I think food should have flavor and texture an...</td>\n",
       "      <td>0</td>\n",
       "      <td>i think food flavor textur lack .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>Appetite instantly gone.</td>\n",
       "      <td>0</td>\n",
       "      <td>appetit instant gone .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>Overall I was not impressed and would not go b...</td>\n",
       "      <td>0</td>\n",
       "      <td>overal i impress would go back .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>The whole experience was underwhelming, and I ...</td>\n",
       "      <td>0</td>\n",
       "      <td>the whole experi underwhelm , i think ll go ni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>Then, as if I hadn't wasted enough of my life ...</td>\n",
       "      <td>0</td>\n",
       "      <td>then , i n't wast enough life , pour salt woun...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Review  Liked  \\\n",
       "0                             Wow... Loved this place.      1   \n",
       "1                                   Crust is not good.      0   \n",
       "2            Not tasty and the texture was just nasty.      0   \n",
       "3    Stopped by during the late May bank holiday of...      1   \n",
       "4    The selection on the menu was great and so wer...      1   \n",
       "..                                                 ...    ...   \n",
       "995  I think food should have flavor and texture an...      0   \n",
       "996                           Appetite instantly gone.      0   \n",
       "997  Overall I was not impressed and would not go b...      0   \n",
       "998  The whole experience was underwhelming, and I ...      0   \n",
       "999  Then, as if I hadn't wasted enough of my life ...      0   \n",
       "\n",
       "                                        Cleaned_review  \n",
       "0                                 wow ... love place .  \n",
       "1                                         crust good .  \n",
       "2                             not tasti textur nasti .  \n",
       "3    stop late may bank holiday rick steve recommen...  \n",
       "4                        the select menu great price .  \n",
       "..                                                 ...  \n",
       "995                  i think food flavor textur lack .  \n",
       "996                             appetit instant gone .  \n",
       "997                   overal i impress would go back .  \n",
       "998  the whole experi underwhelm , i think ll go ni...  \n",
       "999  then , i n't wast enough life , pour salt woun...  \n",
       "\n",
       "[1000 rows x 3 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "80820797",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer()"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "#initializing the vector.\n",
    "hi_vectorize = CountVectorizer()\n",
    "hi_vectorize.fit(df['Cleaned_review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de4f2caa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['absolut', 'absolutley', 'accid', 'accommod', 'accomod', 'accord',\n",
       "       'account', 'ach', 'acknowledg', 'across', 'actual', 'ad', 'afford',\n",
       "       'after', 'afternoon', 'again', 'ago', 'ahead', 'airlin', 'airport',\n",
       "       'al', 'ala', 'albondiga', 'all', 'allergi', 'almond', 'almost',\n",
       "       'alon', 'also', 'although', 'alway', 'amaz', 'ambianc', 'ambienc',\n",
       "       'amount', 'ampl', 'an', 'and', 'andddd', 'angri', 'ani', 'annoy',\n",
       "       'anoth', 'anticip', 'anymor', 'anyon', 'anyth', 'anytim', 'anyway',\n",
       "       'apart', 'apolog', 'app', 'appal', 'appar', 'appeal', 'appet',\n",
       "       'appetit', 'appl', 'approv', 'are', 'area', 'arepa', 'aria',\n",
       "       'around', 'array', 'arriv', 'articl', 'as', 'ask', 'assur', 'at',\n",
       "       'ate', 'atmospher', 'atmosphere', 'atroci', 'attach', 'attack',\n",
       "       'attent', 'attitud', 'auju', 'authent', 'averag', 'avocado',\n",
       "       'avoid', 'aw', 'away', 'awesom', 'awkward', 'ayc', 'az', 'baba',\n",
       "       'babi', 'bachi', 'back', 'bacon', 'bad', 'bagel', 'bakeri',\n",
       "       'baklava', 'ball'], dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " hi_vectorize.get_feature_names_out()[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e8586d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_vector=hi_vectorize.transform(df['Cleaned_review'])\n",
    "my_array=my_vector.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "60ba4b1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "abc88d34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1000x1623 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 5999 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7eacc57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "91e335a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialising\n",
    "km = KMeans(n_clusters=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c5279fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting a model\n",
    "km_model = km.fit(my_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eb0efb92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making predictions\n",
    "y_km = km.predict(my_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8fac7433",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 4, 1, 1, 1, 4, 0, 0, 0, 1, 1, 2, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 3, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 1, 0, 1, 3, 0, 1,\n",
       "       2, 2, 1, 1, 1, 2, 1, 1, 0, 1, 2, 1, 2, 3, 1, 0, 1, 0, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 3, 3, 1, 1, 1, 3, 1, 1, 1, 4, 1, 1, 1, 3, 1,\n",
       "       1, 1, 4, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 2, 1, 1, 2, 0, 1,\n",
       "       1, 1, 1, 1, 1, 4, 4, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 4, 1, 1,\n",
       "       1, 1, 1, 2, 0, 1, 3, 1, 1, 1, 1, 1, 0, 3, 1, 1, 1, 4, 1, 1, 1, 3,\n",
       "       1, 1, 0, 4, 1, 3, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 4, 1, 1, 0, 1, 1,\n",
       "       1, 1, 0, 1, 1, 1, 1, 1, 2, 1, 0, 1, 1, 2, 1, 3, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 3, 1, 1, 4, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 3, 1, 2, 1,\n",
       "       2, 1, 0, 4, 1, 0, 1, 3, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,\n",
       "       1, 1, 1, 2, 1, 1, 1, 3, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,\n",
       "       1, 2, 0, 0, 1, 1, 1, 3, 1, 1, 2, 1, 0, 1, 1, 1, 1, 1, 1, 4, 3, 3,\n",
       "       1, 1, 1, 1, 1, 3, 1, 1, 0, 3, 3, 1, 1, 1, 1, 1, 0, 1, 1, 4, 0, 1,\n",
       "       1, 3, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 2, 1, 1,\n",
       "       1, 1, 1, 1, 0, 1, 1, 1, 1, 4, 1, 3, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 3, 0, 1, 2, 3, 1, 1, 1, 3, 1, 2, 0, 4, 2, 0, 0, 2, 4, 1, 1,\n",
       "       3, 1, 1, 1, 4, 1, 1, 0, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       2, 1, 1, 1, 1, 2, 1, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1,\n",
       "       1, 3, 1, 1, 1, 1, 3, 1, 1, 1, 3, 1, 1, 1, 2, 1, 1, 1, 1, 0, 1, 1,\n",
       "       2, 1, 1, 1, 3, 4, 1, 4, 1, 1, 1, 2, 3, 1, 1, 1, 1, 1, 1, 0, 1, 1,\n",
       "       3, 4, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 4, 2, 1, 1, 1, 0, 1, 1, 0,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 3, 1, 2, 1, 1, 1, 3,\n",
       "       2, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 3, 1, 3, 1, 1, 2,\n",
       "       0, 1, 1, 1, 1, 1, 4, 1, 1, 0, 1, 1, 2, 0, 1, 0, 2, 1, 1, 1, 4, 1,\n",
       "       1, 3, 2, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 3, 1, 1, 1, 0, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 4, 0, 1, 1, 1, 1, 1, 1, 4, 1, 4, 0, 1, 1, 1, 1,\n",
       "       1, 1, 4, 1, 1, 1, 1, 3, 0, 1, 1, 4, 2, 1, 1, 1, 1, 1, 1, 1, 3, 1,\n",
       "       1, 4, 1, 1, 1, 1, 0, 1, 1, 1, 1, 3, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 2, 0, 1, 3, 1, 0, 1, 1, 1, 3, 1, 0, 1, 1, 2,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 3, 1, 1, 1, 0, 1, 3, 1, 1, 1, 3, 1,\n",
       "       1, 1, 1, 1, 3, 1, 1, 1, 1, 3, 1, 1, 1, 1, 4, 2, 1, 1, 1, 1, 1, 1,\n",
       "       4, 1, 1, 1, 1, 4, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 0, 1, 3, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 2, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 3, 1, 1, 1,\n",
       "       1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 3, 0, 1, 1, 1, 1,\n",
       "       1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 2, 0, 1, 0, 1,\n",
       "       0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 3, 1, 0, 1, 1, 1, 1, 4, 1, 1, 1,\n",
       "       1, 1, 1, 3, 4, 1, 1, 3, 1, 0, 0, 3, 1, 1, 1, 3, 1, 1, 1, 1, 1, 0,\n",
       "       3, 1, 0, 3, 1, 1, 1, 1, 0, 0, 1, 1, 4, 1, 1, 1, 1, 1, 1, 0, 4, 1,\n",
       "       1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 3, 0, 2, 1,\n",
       "       1, 1, 1, 1, 4, 1, 1, 0, 1, 3, 0, 0, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1,\n",
       "       1, 1, 0, 1, 1, 2, 1, 1, 1, 3, 1, 1, 1, 1, 3, 1, 0, 1, 1, 1, 1, 3,\n",
       "       0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 2, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
       "       1, 2, 1, 1, 1, 1, 1, 3, 2, 2])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's see our predictions\n",
    "y_km"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3199b774",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.00970874, ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.00960219, 0.00137174, 0.        , ..., 0.00274348, 0.00548697,\n",
       "        0.00685871],\n",
       "       [0.01960784, 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.01333333, 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "km.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7e94fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
